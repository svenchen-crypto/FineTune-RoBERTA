{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02285e6",
   "metadata": {
    "id": "a02285e6"
   },
   "source": [
    "# Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc5329",
   "metadata": {
    "id": "bdcc5329"
   },
   "source": [
    "Install and import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
   "metadata": {
    "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.9/site-packages (4.51.2)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: evaluate in ./.local/lib/python3.9/site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: peft in ./.local/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: trl in ./.local/lib/python3.9/site-packages (0.16.1)\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.9/site-packages (0.45.5)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.local/lib/python3.9/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.9/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.9/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.local/lib/python3.9/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.9/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: psutil in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.local/lib/python3.9/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: rich in ./.local/lib/python3.9/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.9/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.9/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.9/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from rich->trl) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nvidia-ml-py3 in ./.local/lib/python3.9/site-packages (7.352.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n",
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
   "metadata": {
    "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tc4104/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6e377",
   "metadata": {
    "id": "59d6e377"
   },
   "source": [
    "## Load Tokenizer and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
   "metadata": {
    "id": "21f42747-f551-40a5-a95f-7affb1eba4a3"
   },
   "outputs": [],
   "source": [
    "base_model = \"roberta-base\"\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "id2label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
   "metadata": {
    "id": "9e07f641-bec0-43a6-8c26-510d7642916a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of classess and their names\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e24afd",
   "metadata": {
    "id": "c9e24afd"
   },
   "source": [
    "## Load Pre-trained Model\n",
    "Set up config for pretrained model and download it from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
   "metadata": {
    "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=4,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265839d-a088-4693-8474-862641de11ed",
   "metadata": {
    "id": "f265839d-a088-4693-8474-862641de11ed"
   },
   "source": [
    "## Anything from here on can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7413430-be57-482b-856e-36bd4ba799df",
   "metadata": {
    "id": "e7413430-be57-482b-856e-36bd4ba799df"
   },
   "outputs": [],
   "source": [
    "# Split the original training set\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=640, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652452e3",
   "metadata": {
    "id": "652452e3"
   },
   "source": [
    "## Setup LoRA Config\n",
    "Setup PEFT config and get peft model for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
   "metadata": {
    "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876"
   },
   "outputs": [],
   "source": [
    "# PEFT Config\n",
    "peft_config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias = 'none',\n",
    "    #target_modules = ['query'],\n",
    "    target_modules=['query', 'value'],\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec2739d-76b6-4fde-91c2-0fc49e1884b0",
   "metadata": {
    "id": "6ec2739d-76b6-4fde-91c2-0fc49e1884b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=2, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=2, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a769f54e-05ad-4e3c-aae8-d00d1d9dfb2f",
   "metadata": {
    "id": "a769f54e-05ad-4e3c-aae8-d00d1d9dfb2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "base_model.model.classifier.modules_to_save.default.dense.weight\n",
      "base_model.model.classifier.modules_to_save.default.dense.bias\n",
      "base_model.model.classifier.modules_to_save.default.out_proj.weight\n",
      "base_model.model.classifier.modules_to_save.default.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable parameters:\")\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da45f85c-b016-4c49-8808-6eafa7cb5d1b",
   "metadata": {
    "id": "da45f85c-b016-4c49-8808-6eafa7cb5d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 667,396 || all params: 125,316,104 || trainable%: 0.5326\n"
     ]
    }
   ],
   "source": [
    "print('PEFT Model')\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12284b58",
   "metadata": {
    "id": "12284b58"
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
   "metadata": {
    "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1"
   },
   "outputs": [],
   "source": [
    "# To track evaluation accuracy during training\n",
    "# !pip install scikit-learn\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768b4917-65de-4e55-ae7f-698e287535d4",
   "metadata": {
    "id": "768b4917-65de-4e55-ae7f-698e287535d4"
   },
   "outputs": [],
   "source": [
    "# Setup Training args\n",
    "output_dir = \"results\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to=None,\n",
    "    eval_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    #max_steps=1200,\n",
    "    max_steps=-1,\n",
    "    use_cpu=False,\n",
    "    dataloader_num_workers=4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",  # or \"cosine\"\n",
    "    warmup_ratio=0.1, \n",
    "    gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    ")\n",
    "\n",
    "def get_trainer(model):\n",
    "      return  Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          compute_metrics=compute_metrics,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=eval_dataset,\n",
    "          data_collator=data_collator,\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b848278",
   "metadata": {
    "id": "9b848278"
   },
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515",
   "metadata": {
    "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37300' max='37300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37300/37300 3:38:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.835900</td>\n",
       "      <td>0.353928</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.360346</td>\n",
       "      <td>0.896875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.335058</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.293200</td>\n",
       "      <td>0.335853</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.332459</td>\n",
       "      <td>0.903125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.299226</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>0.343026</td>\n",
       "      <td>0.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.317019</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.312508</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.315575</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.290136</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.294489</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.291787</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.286715</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.312442</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.299281</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.300283</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.282289</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.294187</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.279236</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.260220</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.262553</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.903125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.286512</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.250104</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.240300</td>\n",
       "      <td>0.245352</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.246040</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.251633</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.257626</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.255092</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.251706</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.211800</td>\n",
       "      <td>0.257160</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.276140</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>0.249992</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>0.233502</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.255375</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.228081</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.231757</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.274681</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.232956</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.239227</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.221300</td>\n",
       "      <td>0.241771</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.253436</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.237761</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.241743</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.225493</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.241601</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.227014</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.240339</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.261120</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.257192</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.229280</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.224181</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.217037</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.227082</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.223470</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.213530</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.193500</td>\n",
       "      <td>0.228025</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.219468</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.229530</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.197200</td>\n",
       "      <td>0.220550</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.221920</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.232540</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>0.234187</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.230600</td>\n",
       "      <td>0.218565</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.235683</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.220700</td>\n",
       "      <td>0.230017</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.231094</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.218288</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.247424</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>0.225205</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.217403</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>0.229398</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.235684</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.213900</td>\n",
       "      <td>0.229711</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.214813</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.220600</td>\n",
       "      <td>0.206621</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.215146</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.217891</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.225847</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.227176</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.211840</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.242259</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.222917</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.221407</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>0.251824</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.219247</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.226458</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.214059</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.209330</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.222319</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>0.247490</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.232035</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.197300</td>\n",
       "      <td>0.225990</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.226376</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.223990</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.258489</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>0.241050</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.207763</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.196900</td>\n",
       "      <td>0.203665</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.217884</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>0.224475</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.226512</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.218517</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.228251</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>0.238435</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.189900</td>\n",
       "      <td>0.240720</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.221060</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>0.234320</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>0.213467</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.225649</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.229477</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>0.243515</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.235394</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.231089</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>0.212632</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.222972</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.208176</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.228245</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>0.217966</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.226125</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.224518</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.226897</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>0.232539</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.225597</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.195100</td>\n",
       "      <td>0.224638</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.243619</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.241243</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.225036</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.235895</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.176600</td>\n",
       "      <td>0.212415</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.230835</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.222428</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.223025</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.213989</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.219355</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.220398</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.232836</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.227706</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>0.216452</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.215320</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.216911</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.221711</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.222737</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.227983</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.219710</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.214280</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.220417</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.218669</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.220316</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.241353</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.222691</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.217324</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.222496</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>0.218819</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.159200</td>\n",
       "      <td>0.222749</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>0.217588</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.212963</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.219433</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.214007</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.219190</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.232276</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.226232</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.226256</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.176600</td>\n",
       "      <td>0.222139</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.216548</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.240585</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.216369</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.238596</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.233954</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.224301</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.218377</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.202974</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.226946</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.228810</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.226326</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.211963</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.210162</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.223522</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.216440</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.210332</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.182800</td>\n",
       "      <td>0.217245</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.205792</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.204389</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.208569</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.210977</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.203856</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.212862</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.208223</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.184200</td>\n",
       "      <td>0.205501</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>0.198201</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.216974</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.214962</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.210669</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.221288</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.208235</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.204273</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.214868</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.206541</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.196696</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.198962</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.191100</td>\n",
       "      <td>0.209247</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.213028</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>0.211730</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.200916</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.215936</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.221120</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.229470</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>0.227148</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.233913</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.231665</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.207340</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.218248</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.230666</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.211802</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.199400</td>\n",
       "      <td>0.223663</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.205649</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.210070</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.211523</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.212033</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.218001</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>0.214241</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.155400</td>\n",
       "      <td>0.222145</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.220810</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.184900</td>\n",
       "      <td>0.213167</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.206747</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.216203</td>\n",
       "      <td>0.943750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.213558</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.205764</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.205970</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.209432</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.197839</td>\n",
       "      <td>0.940625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.206544</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.207030</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>0.202237</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.206548</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.211812</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.216510</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.190600</td>\n",
       "      <td>0.209107</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.203783</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.207923</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>0.205903</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.204299</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.209902</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.219795</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.209733</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.204898</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>0.207485</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.209673</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.165500</td>\n",
       "      <td>0.208949</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.200937</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>0.159200</td>\n",
       "      <td>0.204935</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.200103</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.213422</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.213487</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.213110</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.216575</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.212858</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>0.145300</td>\n",
       "      <td>0.212476</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.159900</td>\n",
       "      <td>0.208422</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>0.214187</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.205054</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.217609</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.210899</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.220428</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.141400</td>\n",
       "      <td>0.213938</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.213473</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.215584</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.205947</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.207649</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.165600</td>\n",
       "      <td>0.213021</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.206161</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.213149</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.200671</td>\n",
       "      <td>0.942187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.207121</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.205329</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.199057</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.199466</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.198179</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.193686</td>\n",
       "      <td>0.940625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.199610</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.207195</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.203775</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.204107</td>\n",
       "      <td>0.940625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.202404</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.193750</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.196353</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.198843</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.195172</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.199165</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.196066</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.201182</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.203045</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.209983</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.213653</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.216109</td>\n",
       "      <td>0.940625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.222994</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.220459</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.206774</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.209994</td>\n",
       "      <td>0.942187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>0.208900</td>\n",
       "      <td>0.205751</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.204146</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>0.195562</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>0.184200</td>\n",
       "      <td>0.197117</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.203981</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.203926</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.203485</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.206145</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.204291</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.206264</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>0.206327</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.208633</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.207278</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>0.208427</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.204422</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.206049</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.210266</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.206690</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>0.210905</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.205666</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.205843</td>\n",
       "      <td>0.940625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.206563</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.152700</td>\n",
       "      <td>0.207349</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.203346</td>\n",
       "      <td>0.940625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>0.142900</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>0.152100</td>\n",
       "      <td>0.206763</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.202251</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34100</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.202259</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>0.201853</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34300</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>0.204058</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>0.204246</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.201228</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.200493</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34700</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.198141</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.198240</td>\n",
       "      <td>0.940625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34900</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.200778</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.201243</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.203808</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.204007</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35300</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.203781</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.203223</td>\n",
       "      <td>0.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.204869</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.206428</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.207538</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.207266</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35900</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>0.206599</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.207110</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36100</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.204936</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.203309</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.203170</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.203049</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>0.203115</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36700</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.202204</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.201982</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.201812</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.202270</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37100</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.202482</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.202386</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37300</td>\n",
       "      <td>0.174900</td>\n",
       "      <td>0.202368</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_lora_finetuning_trainer = get_trainer(peft_model)\n",
    "\n",
    "result = peft_lora_finetuning_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183be7e-514f-4e64-a6f4-314a827e6be5",
   "metadata": {
    "id": "5183be7e-514f-4e64-a6f4-314a827e6be5"
   },
   "source": [
    "## Evaluate Finetuned Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038198cf-0953-47e7-bd47-b073d05f8378",
   "metadata": {
    "id": "038198cf-0953-47e7-bd47-b073d05f8378"
   },
   "source": [
    "### Performing Inference on Custom Input\n",
    "Uncomment following functions for running inference on custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88ad420-3f46-4eff-9d71-0ce388163062",
   "metadata": {
    "id": "f88ad420-3f46-4eff-9d71-0ce388163062"
   },
   "outputs": [],
   "source": [
    "def classify(model, tokenizer, text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "    prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "    print(f'\\n Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n",
    "    return id2label[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc52bb94-5e13-4943-9225-a6d7fd053579",
   "metadata": {
    "id": "fc52bb94-5e13-4943-9225-a6d7fd053579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Class: 1, Label: Sports, Text: Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\n",
      "\n",
      " Class: 2, Label: Business, Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindlinand of ultra-cynics, are seeing green again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Business'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify( peft_model, tokenizer, \"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\")\n",
    "classify( peft_model, tokenizer, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3e276-bf8c-4403-8a48-5ef19f2beccf",
   "metadata": {
    "id": "68a3e276-bf8c-4403-8a48-5ef19f2beccf"
   },
   "source": [
    "### Run Inference on eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5",
   "metadata": {
    "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "    \"\"\"\n",
    "    Evaluate a PEFT model on a dataset.\n",
    "\n",
    "    Args:\n",
    "        inference_model: The model to evaluate.\n",
    "        dataset: The dataset (Hugging Face Dataset) to run inference on.\n",
    "        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n",
    "                         If False, only predictions will be returned.\n",
    "        batch_size (int): Batch size for inference.\n",
    "        data_collator: Function to collate batches. If None, the default collate_fn is used.\n",
    "\n",
    "    Returns:\n",
    "        If labelled is True, returns a tuple (metrics, predictions)\n",
    "        If labelled is False, returns the predictions.\n",
    "    \"\"\"\n",
    "    # Create the DataLoader\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    if labelled:\n",
    "        metric = evaluate.load('accuracy')\n",
    "\n",
    "    # Loop over the DataLoader\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        # Move each tensor in the batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "\n",
    "        if labelled:\n",
    "            # Expecting that labels are provided under the \"labels\" key.\n",
    "            references = batch[\"labels\"]\n",
    "            metric.add_batch(\n",
    "                predictions=predictions.cpu().numpy(),\n",
    "                references=references.cpu().numpy()\n",
    "            )\n",
    "\n",
    "    # Concatenate predictions from all batches\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "    if labelled:\n",
    "        eval_metric = metric.compute()\n",
    "        print(\"Evaluation Metric:\", eval_metric)\n",
    "        return eval_metric, all_predictions\n",
    "    else:\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809635a6-a2c7-4d09-8d60-ababd1815003",
   "metadata": {
    "id": "809635a6-a2c7-4d09-8d60-ababd1815003"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 80/80 [00:05<00:00, 15.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric: {'accuracy': 0.9359375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check evaluation accuracy\n",
    "_, _ = evaluate_model(peft_model, eval_dataset, True, 8, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf53a3fc-ece4-4709-8f26-134167e96269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in ./.local/lib/python3.9/site-packages (1.7.4.2)\n",
      "Requirement already satisfied: bleach in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (2.10)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.9/site-packages (from kaggle) (6.30.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in ./.local/lib/python3.9/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.9/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (52.0.0.post20210125)\n",
      "Requirement already satisfied: six>=1.10 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in ./.local/lib/python3.9/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (1.26.6)\n",
      "Requirement already satisfied: webencodings in /share/apps/pyenv/py3.9/lib/python3.9/site-packages (from kaggle) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b021d6bc-84d5-4944-99be-3d2e6fabdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.expanduser(\"~/.local/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f211a41a-d14c-43d5-8b86-5e4b1c273e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tc4104/.local/bin/kaggle\n"
     ]
    }
   ],
   "source": [
    "!which kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "kMJgvV1ZnVhd",
   "metadata": {
    "id": "kMJgvV1ZnVhd"
   },
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcdc036c-9a26-4946-8679-467ce46a91ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API 1.7.4.2\n"
     ]
    }
   ],
   "source": [
    "!kaggle --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14638f9c-fcf3-45e4-846d-88c09666bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9ba7198-0b8a-4d05-80bd-1fe15ffd9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c deep-learning-spring-2025-project-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43ebe8a8-cf1c-4f1e-ae12-c9a53a005bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.bash_history', '.triton', '.singularity', 'deep-learning-spring-2025-project-2.zip', '.cache', 'miniconda.sh', 'results', 'train.log', 'DLSP25-Project1', '.viminfo', '.nv', '.jupyter', '.local', '.lesshst', '.ipython', '.conda', '.ipynb_checkpoints', '.config', 'cifar_test_nolabel.pkl', 'Starter_Notebook.ipynb', 'cifar-10-python', 'data', '.ssh', '.kaggle', 'predictions.csv', '.bashrc', 'deep-learning-spring-2025-project-1.zip', 'ondemand']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b541bb6-fc23-4e84-92c7-c1505506c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"deep-learning-spring-2025-project-2.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203",
   "metadata": {
    "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203"
   },
   "source": [
    "### Run Inference on unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7",
   "metadata": {
    "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_unlabelled.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load your unlabelled data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m unlabelled_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_unlabelled.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m unlabelled_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m unlabelled_dataset\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_unlabelled.pkl'"
     ]
    }
   ],
   "source": [
    "#Load your unlabelled data\n",
    "unlabelled_dataset = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "unlabelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60991d3-38b1-4657-8854-408ce66f6b84",
   "metadata": {
    "id": "e60991d3-38b1-4657-8854-408ce66f6b84"
   },
   "outputs": [],
   "source": [
    "# Run inference and save predictions\n",
    "preds = evaluate_model(peft_model, test_dataset, False, 8, data_collator)\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(preds)),\n",
    "    'Label': preds.numpy()  # or preds.tolist()\n",
    "})\n",
    "df_output.to_csv(os.path.join(output_dir,\"inference_output.csv\"), index=False)\n",
    "print(\"Inference complete. Predictions saved to inference_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
